Alright, ladies and gentlemen.
Let's work through this together, shall we?
We will build our first main.cu, sgemm.h and cudaerror.h using meson.
Meson worked great, very happy indeed!
Didn't even need to specifically import the CUDA module.

Alright, let's talk flops.
SGEMM : C = a*A*B + b*C;
A*B: N*N * (A[i,k]*B[k,j]) = N*N*2*N = 2*N^3
A*B+C: 2*N^3 + N^2

And memory:
Loads : 3*N^2
Writes: N^2
		-> 4*N^2

MEMORY COALESCING:
I decided to write the kernels in my own way.
My naive version already kind of deals with the whole coalesced memory access thingy.
The numbers are:
	--> 3.7 GB/s
	--> 943.5 GFLOPS/s

SHARED MEMORY CACHEBLOCKING:
This is a weird one.
We need a loop that walks over the blocks and a loop that walks within the blocks.
I thought I had understood it, but apparently no. 
My version is much slower than the coalesced one :/
Actually, after a lot of debugging:
	--> 5 GB/s
	--> 1298.5 GFLOPS/s
Which is still very slow, but I guess thats expected for a flops-bound operation?
I think I will try the pointer aritmetic magic from the article ... could be neat.
Actually, it went slower. 
Let's keep reading the article and then proceed to the next optimisation.

1D BLOCKTILING:
Uhhh, lets just try to implement it as it is.
I understand how they are doing it, but not why it inceases performance.
I will need to calculate that for my own.
Okay, so it compiles, but returns wrong results. 
As a matter of fact the results are random ... so most likely a memory access outside of the allocated space.
They are correct for all ones in the matrix, but not for random numbers.
This is curious.
Never mind, it is not correct for all ones.
Never mind, it is correct now. The last line really screwed me over.
Alright, the numbers are (N=2048):
	--> 14 GB/s
	--> 3564.6 GFLOPS/s
My notebook gpu has 15.97 TFLOPS/s and 448 GB/s
I think it might be beneficial to also implement one kernel for variable BM ... lets do that.
I decided not to do it, since it becomes a bit ugly regarding loading the shared mem

2D BLOCKTILING:
Implemented and works :) But one needs to setup meson with:
	$ meson setup build --buildtype=release
Alright, lets completely redo all the benchmarks.
For T=float, N=2048:
	--> 26 GB/s
	--> 6666.6 GFLOPS/s
Not too bad ... thats actually like 30% of the total compute power.

VECTORIZED SMEM AND GMEM ACCESSES:
Okay, lets first transpose sA ... should not be too difficult.
Actually, it was very simple.
The benchmarks I will do later.
Lets continue with the text.
Alright, so float4 works pretty straight forward.
It is nice to force GMEM vectorized accesses like that ... could be that my code could profit from that.
